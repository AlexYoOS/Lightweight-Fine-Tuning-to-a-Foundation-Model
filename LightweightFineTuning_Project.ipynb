{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "This notebook demonstrates the implementation of a computationally efficient method to customize an open-source project for a specific use case. While the demonstrated task is sentiment classification, any task available on HuggingFace can be inferred, trained, or customized using the platform's available task-specific models.\n",
    "\n",
    "* PEFT technique: LoRA (Low-Rank Adaptation)\n",
    "* Model: [bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased)\n",
    "* Evaluation approach: Accuracy and Inference Sanity Checking\n",
    "* Fine-tuning dataset: [stanfordnlp/sentiment140](https://huggingface.co/datasets/stanfordnlp/sentiment140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23afe627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, pipeline\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc9bdd",
   "metadata": {},
   "source": [
    "## Loading Bert Model\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed for natural language understanding tasks, and **particularly primed for Sentiment Analysis tasks**. It uses a bidirectional approach to read text, meaning it considers the context from both the left and right sides of a word, providing a deeper understanding of context. \n",
    "\n",
    "## Sentiment140 Dataset from StanfordNLP\n",
    "Sentiment140 is a good dataset for demonstrating and building sentiment analysis models for several reasons:\n",
    "\n",
    "**Size:** It contains 1.6 million labeled tweets, providing a large amount of data for training robust models.<br>\n",
    "**Real-world Data:** The dataset consists of tweets, which are real-world, noisy text data, making it a good representation of actual user-generated content.<br>\n",
    "**Binary Sentiment Labels:** It has clear binary sentiment labels (positive and negative), simplifying the classification task.<br>\n",
    "**Preprocessed:** The dataset is preprocessed to remove common noise in tweets (like URLs and usernames), making it easier to work with.<br>\n",
    "**Publicly Available:** It is freely available, allowing easy access for experimentation and learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b461a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9807da50d43413ab80309214c44026f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/81.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b655e89544ba4aa3b3f702ddf3c28546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1600000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19638a4c73ca4e0d8b05306c12262659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1f89d7ac944eb7bd7b88777c7301bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8d8f88afeb4a1880406cea1edef54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4e63cb3f5b401385d691f0c185c3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f07aef0e274968a06d1827d203ae7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/359 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Specify the model name\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Load the model and tokenizer, adapt model output to number of classes\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the Sentiment140 dataset\n",
    "dataset = load_dataset(\"sentiment140\")\n",
    "\n",
    "# Use a subset of the training dataset for quicker experimentation\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(10000))\n",
    "test_dataset = dataset[\"test\"]  # only contains 498 rows\n",
    "\n",
    "# Define the mapping for the remaining labels - the training dataset has a \"neutral\" label of score 2 added,\n",
    "# these rows are filtered out, since the test dataset only has positive and negative labels \n",
    "mapping = {4: 1, 0: 0}\n",
    "\n",
    "# Function to filter and map sentiment values\n",
    "def filter_and_map_sentiment(example):\n",
    "    if example['sentiment'] in mapping:\n",
    "        example['sentiment'] = mapping[example['sentiment']]\n",
    "        return example\n",
    "    return None\n",
    "\n",
    "# Apply the filtering and mapping to the training dataset\n",
    "filtered_train_dataset = small_train_dataset.filter(lambda x: x['sentiment'] in mapping)\n",
    "filtered_train_dataset = filtered_train_dataset.map(filter_and_map_sentiment)\n",
    "\n",
    "# Apply the mapping to the test dataset\n",
    "test_dataset = test_dataset.filter(lambda x: x['sentiment'] in mapping)\n",
    "test_dataset = test_dataset.map(filter_and_map_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d3d1a9",
   "metadata": {},
   "source": [
    "Check if the **neutral** sentiment rows are deleted in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c71d3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sentiments in the training dataset: [0, 1]\n",
      "Unique sentiments in the test dataset: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Verify the changes\n",
    "\n",
    "# Print the unique values of the 'sentiment' list in the filtered_train_dataset\n",
    "unique_train_sentiments = list(set(filtered_train_dataset['sentiment']))\n",
    "print(\"Unique sentiments in the training dataset:\", unique_train_sentiments)\n",
    "\n",
    "# Print the unique values of the 'sentiment' list in the test_dataset\n",
    "unique_test_sentiments = list(set(test_dataset['sentiment']))\n",
    "print(\"Unique sentiments in the test dataset:\", unique_test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7539e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75d8b97d45144e6acdf7f692bf822f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ab7084464c431db2e5f6a11b369831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/359 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train_dataset = filtered_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare the dataset for training - standardize the column names and set the format to PyTorch tensors\n",
    "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"sentiment\", \"labels\")\n",
    "tokenized_test_dataset = tokenized_test_dataset.rename_column(\"sentiment\", \"labels\")\n",
    "tokenized_train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23ba740",
   "metadata": {},
   "source": [
    "### Instantiate a Trainer Class\n",
    "The trainer class is instantly used for evaluating the foundation model, and later used to train the PEFT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 12:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.707639217376709,\n",
       " 'eval_accuracy': 0.48746518105849584,\n",
       " 'eval_runtime': 11.3483,\n",
       " 'eval_samples_per_second': 31.635,\n",
       " 'eval_steps_per_second': 2.027}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Create a Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d754344f",
   "metadata": {},
   "source": [
    "### Alternative manual evaluation function\n",
    "The trainer class above and the custom function below, both can be used for benchmarking the model.\n",
    "Another HuggingFace library that can be used is the [Evaluate libary](https://huggingface.co/docs/evaluate/en/index), the convenience of this approach was to benchmark not only other metrics like f1-score - which is a necessity for training with **imbalanced datasets** - but also task specific metrics like **BLEU**, **Rouge**, and **Meteor** for evaluating Machine Translations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f24a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenized_test_dataset, device):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of an untrained model on the tokenized test dataset.\n",
    "\n",
    "    Args:\n",
    "        model: The model to be evaluated.\n",
    "        tokenized_test_dataset: The tokenized test dataset.\n",
    "        device: The device to run the evaluation on (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        accuracy: The accuracy of the model on the test dataset.\n",
    "    \"\"\"\n",
    "    # Convert the tokenized input samples to tensors\n",
    "    input_ids = tokenized_test_dataset[\"input_ids\"]\n",
    "    attention_mask = tokenized_test_dataset[\"attention_mask\"]\n",
    "    labels = tokenized_test_dataset[\"labels\"]\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        model.to(device)\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "        # Create a progress bar\n",
    "        progress_bar = tqdm(range(len(input_ids)), desc=\"Evaluation Progress\")\n",
    "\n",
    "        # Initialize an empty list to store predicted labels\n",
    "        predicted_labels = []\n",
    "\n",
    "        # Iterate over the input samples with the progress bar\n",
    "        for i in progress_bar:\n",
    "            # Get the input sample\n",
    "            sample_input_ids = input_ids[i]\n",
    "            sample_attention_mask = attention_mask[i]\n",
    "\n",
    "            # Forward pass for the current input sample\n",
    "            outputs = model(sample_input_ids.unsqueeze(0), attention_mask=sample_attention_mask.unsqueeze(0))\n",
    "\n",
    "            # Get the predicted label for the current input sample\n",
    "            predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "            # Append the predicted label to the list\n",
    "            predicted_labels.append(predicted_label)\n",
    "\n",
    "    # Convert the predicted labels to a tensor\n",
    "    predicted_labels = torch.tensor(predicted_labels)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = (predicted_labels == labels).float().mean().item()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb2c5a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Progress: 100%|██████████| 359/359 [00:12<00:00, 29.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48746517300605774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function call for calculating accuracy \n",
    "accuracy = evaluate_model(model, tokenized_test_dataset, device)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a38ecf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the untrained base models accuracy for later comparison \n",
    "base_model_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "Building a Lora configuration for Bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2d45907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# targeting the \"query\" and \"value\" modules for the lora configuration \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LoRA Configuration\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "    \n",
    "# Step 4: Apply the LoRA Configuration\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4a6002",
   "metadata": {},
   "source": [
    "**Note**: The get_peft_model function is part of the PEFT (Parameter-Efficient Fine-Tuning) framework, which is designed to fine-tune large pre-trained models efficiently. **This function automatically freezes the layers of the foundation model and allows only the LoRA (Low-Rank Adaptation) adapter layers to be trained.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): BertForSequenceClassification(\n",
      "      (bert): BertModel(\n",
      "        (embeddings): BertEmbeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): BertEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(\n",
      "                    in_features=768, out_features=768, bias=True\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(\n",
      "                    in_features=768, out_features=768, bias=True\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): BertPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# bert-base-uncased model including the unfrozen and trainable lora adapter layers (weights and biases)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 58:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>0.652615</td>\n",
       "      <td>0.623955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.640800</td>\n",
       "      <td>0.527348</td>\n",
       "      <td>0.788301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.422427</td>\n",
       "      <td>0.857939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.505400</td>\n",
       "      <td>0.393537</td>\n",
       "      <td>0.855153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>0.387484</td>\n",
       "      <td>0.855153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3125, training_loss=0.5680040612792969, metrics={'train_runtime': 3487.198, 'train_samples_per_second': 14.338, 'train_steps_per_second': 0.896, 'total_flos': 1.32008512512e+16, 'train_loss': 0.5680040612792969, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b47abf88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert_base_uncased-sentiment140/tokenizer_config.json',\n",
       " 'bert_base_uncased-sentiment140/special_tokens_map.json',\n",
       " 'bert_base_uncased-sentiment140/vocab.txt',\n",
       " 'bert_base_uncased-sentiment140/added_tokens.json',\n",
       " 'bert_base_uncased-sentiment140/tokenizer.json')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save function that will save the trained lora weights to a small checkpoint:\n",
    "model.save_pretrained(\"bert_base_uncased-sentiment140\")\n",
    "tokenizer.save_pretrained(\"bert_base_uncased-sentiment140\")\n",
    "\n",
    "# Extract the classifier head state dictionary\n",
    "classifier_state_dict = {\n",
    "    \"classifier.weight\": model.classifier.weight.cpu().detach().numpy(),\n",
    "    \"classifier.bias\": model.classifier.bias.cpu().detach().numpy()\n",
    "}\n",
    "\n",
    "# Save the classifier head state dictionary\n",
    "torch.save(classifier_state_dict, \"bert_base_uncased-sentiment140/classifier_head.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "Comparing the base models accuracy metric to the accuracy of the trained PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# loading saved model from checkpoint\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert_base_uncased-sentiment140\")\n",
    "\n",
    "# Load the base model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert_base_uncased-sentiment140\")\n",
    "\n",
    "# Load the classifier head state dictionary\n",
    "classifier_state_dict = torch.load(\"bert_base_uncased-sentiment140/classifier_head.pth\")\n",
    "\n",
    "# Load the classifier head weights into the model\n",
    "model.classifier.weight.data = torch.tensor(classifier_state_dict[\"classifier.weight\"])\n",
    "model.classifier.bias.data = torch.tensor(classifier_state_dict[\"classifier.bias\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f0aa5b60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Progress: 100%|██████████| 359/359 [00:12<00:00, 28.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model Accuracy: 0.8551532030105591\n",
      "Untrained Base Model Accuracy: 0.48746517300605774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function call for calculating accuracy \n",
    "accuracy = evaluate_model(model, tokenized_test_dataset, device)\n",
    "print(\"PEFT Model Accuracy:\", accuracy)\n",
    "print(\"Untrained Base Model Accuracy:\", base_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af1171e",
   "metadata": {},
   "source": [
    "**Note**: Conventionally if using AutoModel functionality for loading a PEFT model, the specific PEFT version of Automodel is supposed to be used, in this case **AutoPeftModelForSequenceClassification**, however since the **classifier-head layers** which where adapted to the dataset where randomly initialized ad not loaded correctly- presumably because the PEFT library or the specific implementation of LoRA might not fully integrate with the Hugging Face transformers library's save and load mechanisms - the layers weights and biases had to manually be saved and loaded into the reloaded model, which is only possible with default **AutoModelForSequenceClassification**. <br>\n",
    "\n",
    "Issues as such occur regularly with opensource, and unrestricted models. Domain-specific knowledge must be applied to **patch** these hurdles.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f425a90b",
   "metadata": {},
   "source": [
    "## Sanity Check\n",
    "\n",
    "Performing Inference with Custom Trained PEFT model demonstrating prediction samples from test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: @stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.\n",
      "Predicted Label: LABEL_1\n",
      "Score: 0.8558\n",
      "--------------------------------------------------\n",
      "Text: Reading my kindle2...  Love it... Lee childs is good read.\n",
      "Predicted Label: LABEL_1\n",
      "Score: 0.8653\n",
      "--------------------------------------------------\n",
      "Text: Ok, first assesment of the #kindle2 ...it fucking rocks!!!\n",
      "Predicted Label: LABEL_1\n",
      "Score: 0.8620\n",
      "--------------------------------------------------\n",
      "Text: @kenburbary You'll love your Kindle2. I've had mine for a few months and never looked back. The new big one is huge! No need for remorse! :)\n",
      "Predicted Label: LABEL_1\n",
      "Score: 0.6829\n",
      "--------------------------------------------------\n",
      "Text: @mikefish  Fair enough. But i have the Kindle2 and I think it's perfect  :)\n",
      "Predicted Label: LABEL_1\n",
      "Score: 0.8499\n",
      "--------------------------------------------------\n",
      "Text: @richardebaker no. it is too big. I'm quite happy with the Kindle2.\n",
      "Predicted Label: LABEL_0\n",
      "Score: 0.7457\n",
      "--------------------------------------------------\n",
      "Text: Fuck this economy. I hate aig and their non loan given asses.\n",
      "Predicted Label: LABEL_0\n",
      "Score: 0.7897\n",
      "--------------------------------------------------\n",
      "Text: Jquery is my new best friend.\n",
      "Predicted Label: LABEL_1\n",
      "Score: 0.8672\n",
      "--------------------------------------------------\n",
      "Text: Loves twitter\n",
      "Predicted Label: LABEL_1\n",
      "Score: 0.8017\n",
      "--------------------------------------------------\n",
      "Text: how can you not love Obama? he makes jokes about himself.\n",
      "Predicted Label: LABEL_0\n",
      "Score: 0.6844\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def infer_sentiment(lora_model, tokenizer, test_dataset, device, num_samples=10):\n",
    "    \"\"\"\n",
    "    Perform inference on rows of the test dataset for sanity checking or demonstration.\n",
    "\n",
    "    Args:\n",
    "        lora_model: The trained LoRA model.\n",
    "        tokenizer: The tokenizer used for the model.\n",
    "        test_dataset: The tokenized test dataset.\n",
    "        device: The device to run the inference on (e.g., 'cuda' or 'cpu').\n",
    "        num_samples: The number of samples to infer for demonstration.\n",
    "\n",
    "    Returns:\n",
    "        results: A list of dictionaries containing the text and predicted label for each sample.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    lora_model.eval()\n",
    "    lora_model.to(device)\n",
    "\n",
    "    # Extract text data from the test dataset\n",
    "    texts = test_dataset[\"text\"][:num_samples]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            # Tokenize the input text\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "            # Perform inference\n",
    "            outputs = lora_model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Get the predicted label and score\n",
    "            predicted_label = torch.argmax(logits, dim=1).item()\n",
    "            score = torch.softmax(logits, dim=1).max().item()\n",
    "\n",
    "            # Map the predicted label to the corresponding class name\n",
    "            label_name = lora_model.config.id2label[predicted_label]\n",
    "\n",
    "            # Append the result\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"label\": label_name,\n",
    "                \"score\": score\n",
    "            })\n",
    "\n",
    "            # Display the result\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Predicted Label: {label_name}\")\n",
    "            print(f\"Score: {score:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example function call\n",
    "results = infer_sentiment(model, tokenizer, dataset[\"test\"], device, num_samples=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
